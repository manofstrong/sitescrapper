<?php 
/**
 *  SiteScrapper class
 *
 *  @license https://opensource.org/licenses/MIT MIT license
 *  @link https://github.com/manofstrong/sitescrapper
 *
 *  @author manofstrong
 */

namespace Manofstrong;

use Medoo\Medoo;
use Goutte\Client;
use vipnytt\SitemapParser;
use DonatelloZa\RakePlus\RakePlus;
use ExtractContent\ExtractContent;
use vipnytt\SitemapParser\Exceptions\SitemapParserException;

class SiteScrapper{

   /**
    * Accepted Webpage Mime Type
    *
    * HTML files are the only accepted external urls. However, the class SiteScrapper
    * might be able to handle other types such as text (text/*) based types
	* You can change this when working with other mime types.
	*
	* @var string
    */
    protected const ACCEPTED_WEBPAGE_MIME_TYPE = 'text/html';	
	
   /**
    * The Default User Agent
	*
	* @var string 
    */
    protected const DEFAULT_USER_AGENT = 'ManofStrong SiteScrapper Tool v1.0 (+https://github.com/manofstrong/sitescrapper/blob/master/README.md)';
	
   /**
    * User-Agent to send with every HTTP(S) request
	*
    * @var string
    */
    protected $userAgent;

   /**
    * Database Credentials Array
    * @var array
    */
	protected $databaseArray = [];
		
   /**
	*
	* Generates a unique ID for the database. 
	*
	* Credit to user hackan@gmail.com at php.net/manual/en/function.uniqid.php for this nifty function,
	* it is a better alternative to uniqid() which can lead to repeated IDs.
	*
	* @param int $length A number of the digits the unique ID should have. This can be modified to an number.
	*
	* @return string
	*/		
	protected function uniqidReal($length = 10) {
		if (function_exists("random_bytes")) {
			$bytes = random_bytes(ceil($length / 2));
		} elseif (function_exists("openssl_random_pseudo_bytes")) {
			$bytes = openssl_random_pseudo_bytes(ceil($length / 2));
		} else {
			throw new Exception("no cryptographically secure random function available");
		}
		return substr(bin2hex($bytes), 0, $length);
	}
		
   /**
	* Getter function to read the userAgent 
	*/
	public function getUserAgent(){
		return $this->userAgent;
	}
	
   /**
	* Setter function to change the userAgent
	*
	* @param string $newUserAgent
	*/
	public function setUserAgent($newUserAgent){
		$this->userAgent = $newUserAgent;
	}
	
   /**
	* Changes the UserAgent if one is provided by user else sticks to the default user agent.
	* 
	* @return string
	*/
	protected function finalUserAgent(){
		
		$i = $this -> getUserAgent();
		if ($i == '' || !$i){
			$newUserAgent = $this::DEFAULT_USER_AGENT;
		}
		else{
			$newUserAgent = $this -> getUserAgent();
		}
		
		return $newUserAgent;
	}
	
   /**
	* Takes the database credentials as provided by user and pushes them to the databaseArray const.
	*
	* @param string $dbName
	* @param string $host
	* @param string $username
	* @param string $password
	*
	* @return true meaning that nothing has gone wrong with the data provided.
	*/
	public function databaseCredentials($dbName,$host,$userName,$password){
		
		array_push($this -> databaseArray, $dbName , $host , $userName , $password);
		
		return true;		
	}
	
   /**
	* Uploads the scrapped content into the database.
	* Uses class Medoo\Medoo by @https://github.com/catfan/Medoo
	*
	* @param string $dbName
	* @param string $host
	* @param string $username
	* @param string $password
	*
	* @throws Fatal Error when there is a database error. Provides the error generated by class Medoo\Medoo
	* @return true meaning that nothing has gone wrong with the data provided.
	*/	
	protected function uploadContent($keywords, $title, $content, $wordCount, $url){	
		
		$databaseCredentials = $this -> databaseArray;
		
		$database = new Medoo([
			'database_type' => 'mysql',
			'database_name' => $databaseCredentials['0'],
			'server' => $databaseCredentials['1'],
			'username' => $databaseCredentials['2'],
			'password' =>  $databaseCredentials['3'],
			'charset' => 'utf8',
		]);
		
		$database->insert("site_scrapper", [
			"unique_id" => $this -> uniqidReal(),
			"keywords" => $keywords,
			"title" => $title,
			"content" => $content,
			"wordcount" => $wordCount,
			"url" => $url
		]);
		
		if($database->error()[0] != '00000'){
			trigger_error('There is an issue with the database configuration. Function uploadContent() requires the database configuration to work : ' . $database->error()[2], E_USER_ERROR);
		}else{			
			return true;		
		}
		
	}
	
   /**
	* Extracts content from the webpage provided through.
	* Uses class ExtractContent\ExtractContent by @https://github.com/sters/extract-content
	*
	* @param string $url
	* @return array [$title, $importantContent]
	*/		
	protected function extractContent($url){
		$client = new Client();
		
		$crawler = $client->request('GET', $url);
		
		$html_tag = "html";
		
		$title_tag = "title";
		
		$title = $crawler->filter($title_tag)->html();		
		
		$title = ltrim(rtrim($title));

		$htmlBodyContent = $crawler->filter($html_tag)->html();
		
		$extractor = new ExtractContent($htmlBodyContent);
		
		$importantContent = $extractor->analyse();
		
		return [$title, implode("\r\n", $importantContent)];
	}
	
   /**
	* Validates that the webpage of the link provided is really online by checking headers for 200 status code.
	*
	* @param string $url
	*
	* @return false if the header status is not 200 meaning it is offline or there is an error
	*/
	protected function validateWebPage($url){		
		$headers  = get_headers($url);
		
		$code = substr($headers[0], 9, 3);		
				
		$mime = implode(' ',$headers);;		
		
		$mime_status = strpos($mime, $this::ACCEPTED_WEBPAGE_MIME_TYPE);
				
		if (!$mime_status || $code != '200' ){
			return false;
		}
		
	}
	
   /**
	* Gets the wordcount of the extracted content
	*
	* @param string $content
	* @return int $wordcount
	*/
	protected function wordCount($content){
		$wordcount = str_word_count($content);
		
		return $wordcount;
	}
	
	
   /**
	* Gets the wordcount of the extracted content
	*
	* @param string $text
	* @return string $finalKeywords
	*/
	protected function keywordGenerator($text){
		
		$keywords = RakePlus::create($text, 'en_US', 3)->keywords();
		
		$finalKeywords = implode (", ", $keywords);
		
		return $finalKeywords;
		
	}
	
   /**
	* Extracts sitemaps from the url provided other functions
	* Uses class use vipnytt\SitemapParser by @https://github.com/VIPnytt/SitemapParser
	*
	* @param string $stringUrl
	* @throws vipnytt\SitemapParser\SitemapParserException
	* @return string
	*/	
	public function singleSiteMap($singleUrl){		
	
		if (0 == count($this -> databaseArray)){
			trigger_error('You need to provide database credentials through method databaseCredentials().', E_USER_ERROR);
		}
	
		try {
			
			$parser = new SitemapParser($this->finalUserAgent());
			$parser->parseRecursive($singleUrl);
			
			foreach ($parser->getURLs() as $url => $tags) {
				
				echo 'URL: ' . $url . ' ...................';
				
				if ($this -> validateWebPage($url) === false ){
					trigger_error('This page ' . $url . ' is not a valid webpage so it has been skipped.', E_USER_NOTICE);
					continue;					
				}
				
				$content = $this -> extractContent($url);
				
				$title = $content[0];
				
				$bodyText = $content[1];			
			
				$keywords = $this -> keywordGenerator($bodyText);
				
				$wordCount = $this -> wordCount($bodyText);
				
				echo '...................';
				
				$this ->uploadContent($keywords, $title, $bodyText, $wordCount, $url);				
				
				echo "Successfully Uploaded\r\n";
				
			}
			
		} catch (SitemapParserException $e) {
			echo $e->getMessage(); die;
		}
	}
	
	
   /**
	* Extracts sitemaps from the url provided other functions
	* Uses class use vipnytt\SitemapParser by @https://github.com/VIPnytt/SitemapParser
	* Basically a demo of the content, does not upload the content.
	*
	* @param string $stringUrl
	* @param string $numberOfPages
	* @throws vipnytt\SitemapParser\SitemapParserException
	* @return string
	*/	
	public function showContentSiteMap($singleUrl, $numberOfPages){	
		
		if(!is_int($numberOfPages)){
			trigger_error('Function showContentSiteMap() requires the second value to be an interger.', E_USER_ERROR);
		};
	
		try {
			
			$parser = new SitemapParser($this->finalUserAgent());
			$parser->parseRecursive($singleUrl);
			
			$all_content = [];
			
			$i = 1;
			
			foreach ($parser->getURLs() as $url => $tags) {				
				
				if ($i > $numberOfPages) {
					break;
				}
				
				if ($this -> validateWebPage($url) === false ){
					trigger_error('This page ' . $url . ' is not a valid webpage so it has been skipped.', E_USER_NOTICE);
					continue;					
				}
				
				$content = $this -> extractContent($url);
				
				$title = $content[0];
				
				$bodyText = $content[1];			
			
				$keywords = $this -> keywordGenerator($bodyText);
				
				$wordCount = $this -> wordCount($bodyText);
				
				$all_content = ['url' => $url,'title' => $title, 'keywords' => $keywords, 'content' => $bodyText, 'wordcount' => $wordCount];
				
				print_r ($all_content);
				
				$i++;
			}
			
		} catch (SitemapParserException $e) {
			echo $e->getMessage(); die;
		}
	}
	
   /**
	* Obtains sitemaps in the form of an array and then runs them through the function singleSiteMap()
	*
	* @param array $arrayUrl
	* @throws Fatal Error if input provided is not array
	*/	
	public function siteMapsArray($arrayUrl){	
	
		if(!is_array($arrayUrl)){
			trigger_error('Function siteMapsArray() can only take an array', E_USER_ERROR);
		};
		
		foreach ($arrayUrl as &$singleUrl) {
			$this -> singleSiteMap($singleUrl);
		}		
	}
	
   /**	
	* Displays bytes in human readable format when function validateSiteMapFile() has the error of too large files.
	* Credit to user Leo at https://stackoverflow.com/questions/2510434/format-bytes-to-kilobytes-megabytes-gigabytes/30528828 for this nifty function,
	*
	* @param bytes $bytes
	* @returns int string $bytes $units
	*/	
	protected function formatBytes($bytes, $precision = 2) { 
	
		$units = array('B', 'KB', 'MB', 'GB', 'TB'); 

		$bytes = max($bytes, 0); 
		$pow = floor(($bytes ? log($bytes) : 0) / log(1024)); 
		$pow = min($pow, count($units) - 1); 

		$bytes /= pow(1024, $pow);

		return round($bytes, $precision) . ' ' . $units[$pow]; 
	}

   /**
	* Validates the provided sitemap urls to ensure compliance with RFC 2396 for siteMapFile()
	* Lists the invalid urls in a "skippedurls .txt" file to allow the user to corrent them for later scrapping.
	*	
	* @param array $arrayUrl
	* @throws Fatal Error if input provided is not array
	*/	
	protected function validateFileUrls($url){	
	
		if (!filter_var($url, FILTER_VALIDATE_URL)) {
			trigger_error('The url ' .$url . ' is not a valid url according to RFC 2396. It will not be included in the Scrapping. It will be added to skipped urls in the \'skippedeurls.txt\'. You can correct them and start again. ', E_USER_NOTICE);
			
			$skippedUrlsFile = fopen('skippedeurls.txt', 'a');
			
			fwrite($skippedUrlsFile, $url. "\r\n");
			
			fclose($skippedUrlsFile);
		}
	}
	
   /**
	* Validates the provided sitemap file for three errors: whether it exists, it is a simple txt file, and is less than 100MB for siteMapFile()
	*	
	* @param file $file
	* @throws Fatal Error if file does not exist, is not a text file, and larger than 100MB
	*/	
	protected function validateSiteMapFile($file){
		
		if(!file_exists($file)){
			trigger_error('The file ' .$file . ' does not exist', E_USER_ERROR);
		};
		
		if(mime_content_type($file) != 'text/plain'){
			trigger_error('Function siteMapsFile() can only take text file (with mime type \'text/plain\' extension). This file has the following mime type : ' . mime_content_type($file) , E_USER_ERROR);
		}		
		
		if (filesize($file) > 104857600){			
			trigger_error('The function siteMapFile() only accepts files less than 100 MB. The ' .$file . ' is ' . $this -> formatBytes(filesize($file)) . 'Please break up your file into smaller files before proceeding.', E_USER_ERROR);
		}
		
		clearstatcache();		
	}
	
   /**
	* Extracts the sitemap urls in provided txt file. Then loops through each line and uploads them.
	*	
	* @param file $filePath
	* @throws Fatal Error if file does not exist, is not a text file, and larger than 100MB
	*/	
	public function siteMapFile($filePath){		
		
		$this -> validateSiteMapFile($filePath);
		
		$file = new \SplFileObject($filePath);

		while (!$file->eof()) {			
			
			$url = $file->fgets();	
			
			$url = rtrim($url, "\r\n");
			
			$this -> validateFileUrls($url);
			
			$this -> singleSiteMap($url);
		}
	
		$file = null;
	}
	
	
}

?>